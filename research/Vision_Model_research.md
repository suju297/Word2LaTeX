# Document Layout Understanding Models for Word-to-LaTeX Conversion

Converting a Word document (DOCX) to LaTeX requires understanding the document’s layout and structure. Beyond extracting raw text, an effective solution must detect **page regions** – such as headers, footers, body paragraphs, tables, figures, and textboxes – and classify them correctly. By using a “layout oracle” (a model that understands document layouts), we can guide the conversion process to decide when to use LaTeX environments like `\begin{figure}`, `\begin{table}`, or simply leave content as paragraphs. Below, we review state-of-the-art computer vision and multimodal models for document layout analysis, along with their available tools, strengths, and how they can complement DOCX parsing.

## Key Tasks in Document Layout Analysis

* **Region Segmentation & Classification:** Identifying and segmenting the distinct layout zones on a page (e.g. separating a page’s header, footer, main text blocks, tables, figures, etc.) and assigning each a category label. This is often treated as an object detection or segmentation task, outputting bounding boxes/masks with labels[huggingface.co](https://huggingface.co/blog/document-ai#:~:text=Document%20layout%20analysis%20is%20the,boxes%2C%20along%20with%20class%20names). Successful models can accurately label typical elements like paragraphs vs. titles, tables, figures, and lists[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/1908.07836#:~:text=In%20this%20work%2C%20we%20propose,dataset%20can%20be%20a%20more).
* **OCR and Geometry Alignment:** Aligning textual content with the detected regions. In practice, an OCR engine or DOCX parser provides text content and its coordinates, which must be matched to the layout regions. This ensures that, for example, the text recognized within a detected table region is grouped together, or that a figure’s caption text is linked to the figure’s image box. Proper geometry alignment preserves reading order (especially in multi-column layouts) and associates text with the right layout zones.
* **Semantic Role Detection:** Beyond physical layout, understanding the role of text segments (e.g. distinguishing a *figure caption* from a normal paragraph, or a *section heading* from body text). This often involves analyzing text style or keywords (like “Figure 1:” prefix) in combination with layout. A model or heuristic for *style zoning* might classify text segments as titles, subtitles, footnotes, captions, etc., based on visual cues or learned patterns.

These tasks together form the basis of a “layout oracle” that can guide how content is wrapped in LaTeX. For instance, detecting an image region and its caption tells us to use a `figure` environment with an `\includegraphics` and `\caption`. Identifying a table region would trigger a LaTeX `table` with a tabular environment for the content. If two large text regions are detected side by side, that indicates a two-column layout (which might translate to a `multicols` environment or a two-column document class in LaTeX). Headers and footers could be mapped to LaTeX page styles or ignored if not needed. Below we survey prominent models that can perform these layout understanding tasks.

![https://ar5iv.labs.arxiv.org/html/1908.07836](blob:https://chatgpt.com/c1c3675c-d2a2-49a4-83c3-4cb82704b544)

 *Example of automatic document layout annotation.* In the image, the page is segmented into regions: the title/header is marked in red, body text in green, figure images in blue, tables in yellow, and lists in cyan[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/1908.07836#:~:text=Image%3A%20Refer%20to%20caption%20,a). Such region classification provides a structural map of the page, which is crucial for deciding LaTeX environments (e.g. the blue region would become a `figure` in LaTeX, the yellow region a `table`, etc.).

## Notable Models for Document Layout Understanding

Many modern models for layout analysis leverage Transformers and multimodal learning to combine visual and textual information. Others use vision-only CNN or Transformer backbones for object detection on page images. Below are some of the best models and approaches, along with their strengths and integration support:

* **LayoutLM Family (v1, v2, v3):** *LayoutLM* is a series of Transformer models from Microsoft that jointly model text and layout for document understanding.
  * **LayoutLMv1** was text-centric: it takes in OCR-extracted text and 2D coordinates, encoding each token with a BERT-like embedding plus spatial position embedding. It learned to use layout cues (token positions on the page) to improve tasks like form understanding. However, v1 did not directly process the image pixels.
  * **LayoutLMv2** incorporated visual features by adding a CNN backbone to process the page image. It combines text, layout, and image features through cross-modal attention, giving it a richer understanding of document structure. Tasks like classification of fields, token labeling, and relationship extraction improved with this multimodal approach.
  * **LayoutLMv3** unified the architecture by dropping the CNN in favor of patch-based image embeddings (like ViT) and training with *three* objectives: masked text, masked image, and word-patch alignment. This alignment objective forces the model to learn which image regions correspond to which words. LayoutLMv3 achieved  **state-of-the-art results in both text-centric and image-centric tasks** , including document layout analysis. In fact, fine-tuning LayoutLMv3 with a detection head on the PubLayNet dataset yields an mAP around 0.95 in identifying text, titles, tables, etc., topping prior models.
  * *Strengths:* LayoutLM models are excellent for understanding **semantics in context** – e.g., identifying that a certain text block is a title or a caption based on both its content (phrases, formatting) and position on the page. They shine in tasks like form field extraction, where reading order and text relationships matter, and can implicitly learn layout classes when fine-tuned (e.g., classifying each token or text segment as part of a header, list, table, etc.). LayoutLMv3 in particular can act as a general  **layout-aware encoder** , since it ingests the full page image plus text.
  * *Integration:* Pre-trained weights for LayoutLM v1/v2/v3 are available on Hugging Face Transformers (e.g. `microsoft/layoutlmv3-base`), and the models can be fine-tuned for custom tasks. They require an OCR preprocessing step to supply text tokens and bounding boxes (the Hugging Face `LayoutLMv3Processor` can prepare image + text inputs conveniently). For layout classification, one could fine-tune LayoutLM to classify each text segment’s type (paragraph vs header vs footer, etc.) by feeding tokens and training a token or sequence classification head. LayoutLMv3’s visual backbone was pre-trained on PubLayNet’s detection features, so it already has learned about document regions, which is advantageous for layout analysis tasks. Hugging Face and PyTorch support make it relatively easy to integrate, though using it purely as a “layout oracle” might be overkill if you only need region positions – its real power is combining text+layout for nuanced understanding.
* **Document Image Transformer (DiT):** DiT is a pure vision Transformer model (similar to ViT) specifically pre-trained on document images by Microsoft. Instead of using any text input, DiT was self-supervised on 42 million document images (masking patches like BEiT) to learn general document layout patterns. It achieved **state-of-the-art** results when fine-tuned on:
  * Document image classification (e.g., distinguishing forms vs. letters) – DiT beat previous models that used text, reaching ~92.7% on RVL-CDIP.
  * **Document layout analysis** – when fine-tuned on PubLayNet (which has 5 region classes), DiT reached about  **94.9% mAP** . It outperforms a ResNeXt-FPN CNN backbone on the same detection task. DiT can be used as the feature extractor in a Mask R-CNN or similar detection framework[huggingface.co](https://huggingface.co/blog/document-ai#:~:text=Models%20that%20are%20currently%20state,a%20document%20is%20shown%20here).
  * Table detection – fine-tuned on the ICDAR2019 table dataset, it also set new highs[huggingface.co](https://huggingface.co/docs/transformers/v4.28.1/model_doc/dit#:~:text=,images%20and%20240%20testing%20images).
  * *Strengths:* DiT is vision-only, so it’s very useful as a **drop-in replacement for a CNN backbone** in any detection or segmentation pipeline. It excels at purely visual layout cues: e.g., detecting column structures, boundaries of tables and figures, even without textual clues. Because it doesn’t rely on OCR, it can detect non-textual regions (images, separators) effectively. It’s also lighter to deploy if textual analysis isn’t needed – you can run DiT to get bounding boxes of layout elements, then separately run OCR just within those if needed.
  * *Integration:* Hugging Face provides the DiT base model (`microsoft/dit-base`) which can be loaded in Transformers, but for layout analysis you would use it with an object detection head. Microsoft’s implementation (in the UNILM GitHub) integrates DiT with Detectron2 for a Cascade Mask R-CNN detector. In practice, you can use the fine-tuned weights they released for PubLayNet – for example, the model used in a HuggingFace demo Space loads a DiT-base + Cascade RCNN that outputs regions for “text”, “title”, “list”, “table”, “figure”. This pre-trained model (DiT-base Cascade on PubLayNet) is available via the UNILM GitHub and achieves ~95% mAP on PubLayNet. Integration requires installing Detectron2 and the DiT model code, but once set up, it’s straightforward to run on page images (the demo provides an example of using a predictor on an image to get regions). If needing a simpler route, one could export the detector to ONNX (the HF `optimum` package and ONNX runtime could help, as hinted by community models), though this is an advanced deployment step. In summary, DiT is one of the **most powerful layout oracles** available, ideal if you want high-accuracy detection of regions by visual cues.
* **Donut (Document Understanding Transformer):** Donut, developed by Clova AI (NAVER), takes a novel OCR-free approach: it’s an end-to-end model that directly generates a structured output (like JSON or text) from the document image. It combines a Swin Transformer visual encoder with a BART decoder. Donut is trained on tasks such as parsing forms, receipts, or Visual QA by learning to output text content in specific formats (e.g., key-value pairs) from the image alone.
  * *Strengths:* Donut’s advantage is that it bypasses separate OCR – it “reads” and interprets layout in one go. For layout analysis, one could fine-tune Donut to output, say, a markup of the page: for example, XML/LaTeX tags around different regions. In theory, you could train it to produce LaTeX code from the image (like wrapping detected elements in appropriate environments) as a direct output. It’s particularly good for scenarios where text and layout need to be jointly decoded (e.g. extracting a field from a specific box on a form, or associating a figure caption with the figure image without an explicit OCR step). Because the decoder can be conditioned to produce certain tokens, Donut can learn to recognize *and label* regions (for instance, generate a token “`<table>`” when it “sees” a table region).
  * *Integration:* Donut is available in Hugging Face Transformers (`naver-clova-ix/donut-base` and various fine-tuned versions for tasks like DocVQA). It can be used via the high-level pipeline API for document QA or parsing tasks. However, to use it as a layout classifier, one would likely need to fine-tune it on a custom task where the target output is a sequence describing the layout. That might require preparing training data (e.g., take images and create text labels like “`<header>`Header text`</header><para>`Paragraph text...`</para>`”). This is more involved than using off-the-shelf detectors. Pretrained weights exist for certain domains (receipts, invoices), but not explicitly for generic layout zoning – you would leverage the base model and train it for your purpose. If successful, the benefit is a single model that does end-to-end understanding. But the complexity of training and the need for a lot of paired image-markup data means Donut might be a heavier lift for this use-case compared to detection models or LayoutLM. In summary, Donut is powerful and  **flexible in output format** , but using it as a layout oracle would entail custom fine-tuning; it’s likely most useful if you also want to incorporate content extraction with layout (e.g., generating LaTeX directly, not just identifying regions).
* **DocFormer:** DocFormer is a multimodal Transformer model introduced by AWS/Amazon for Visual Document Understanding. Like LayoutLMv2/v3, it fuses text, vision, and spatial features via a specialized self-attention mechanism. A ResNet50 extracts visual features which are flattened into tokens, and text tokens with geometry embeddings are processed in parallel attention streams that exchange information. DocFormer was shown to achieve state-of-the-art results (as of 2021) on several document tasks – for example, it achieved **96% accuracy on RVL-CDIP classification** and beat LayoutLMv2 on form understanding (FUNSD) by a small margin.
  * *Strengths:* DocFormer’s design makes it very effective at tasks requiring  **fine-grained alignment of text and layout** , such as entity extraction from forms or complex documents. Its shared spatial embeddings help correlate words with visual regions. For layout analysis specifically, DocFormer could be fine-tuned to classify each text token or segment into layout categories (similar to LayoutLM). It doesn’t natively output bounding boxes for non-text elements (e.g., image regions), since it still relies on OCR for text features. So it might classify *textual regions* (header vs body vs caption), but identifying pure graphic regions (figures with no OCR text) would be challenging without additional object detection.
  * *Integration:* There is an open implementation of DocFormer (the original or third-party) and the concept is described in the literature, but it’s not as widely supported in frameworks as LayoutLM or DiT. Hugging Face does not have DocFormer in transformers at the time of writing (it focuses on LayoutLM instead). To use DocFormer, one might rely on the authors’ code or check if any community projects have released it. In practice, unless you specifically need DocFormer’s approach, LayoutLMv3 covers a very similar territory (unified text+image transformer) with readily available tools. DocFormer is notable as it pioneered some multimodal attention ideas, but for ease of use, one of the **LayoutLM family or DiT** would likely be more convenient.
* **PubLayNet-based Layout Detectors:** PubLayNet is a large public dataset of document images (360K+ pages from scientific PDFs) automatically annotated for layout regions[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/1908.07836#:~:text=In%20this%20work%2C%20we%20propose,dataset%20can%20be%20a%20more). It includes five classes:  **text, title, list, table, and figure** . Many effective layout models are trained on PubLayNet:
  * The original PubLayNet paper demonstrated that deep object detectors (like Faster R-CNN and Mask R-CNN) trained on this data can “accurately recognize the layout of scientific articles”[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/1908.07836#:~:text=figure%2C%20and%20table,dataset%20can%20be%20a%20more). They reported high precision/recall (around 96-97% for text/figure/table detection in their tests).
  * Pretrained detectors from PubLayNet are  **readily available** . For example, the *LayoutParser* library provides a model zoo with pre-trained Detectron2 models for PubLayNet. One recommended model is a Mask R-CNN with a ResNeXt-101-FPN backbone, which achieves about **88-89% mAP** on the PubLayNet benchmark[layout-parser.readthedocs.io](https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html#:~:text=lp%3A%2F%2FPubLayNet%2Fmask_rcnn_R_50_FPN_3x%2Fconfig%20PubLayNet%20mask_rcnn_X_101_32x8d_FPN_3x%20lp%3A%2F%2FPubLayNet%2Fmask_rcnn_X_101_32x8d_FPN_3x%2Fconfig%2088,csv%20NewspaperNavigator)[layout-parser.readthedocs.io](https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html#:~:text=lp%3A%2F%2FPubLayNet%2Fmask_rcnn_X_101_32x8d_FPN_3x%2Fconfig%2088). These models predict bounding boxes (or masks) for each region and classify it as one of the five categories. Using such a model on a Word page (exported to PDF/image) would directly give coordinates of paragraphs, tables, figures, etc.
  * *Strengths:* These detectors are  **battle-tested for layout zoning** . They excel at identifying the physical regions and are relatively robust due to the large training set. They will detect tables even if they are just drawn with lines, and figures whether they include images or charts. They can also find if text is separated into multiple columns by splitting into separate text regions. However, categories are limited to those five – e.g., there’s no separate “header” class in PubLayNet (a title at the top might be labeled as "Title", but running headers on pages would probably be just part of "Text" or ignored if not enough content). Also, “textboxes” or sidebars would likely fall under the "Text" class unless they were distinctly styled as lists or titles. So, depending on your needs, you might need to post-process or fine-tune to detect headers/footers specifically (perhaps by fine-tuning on a small custom dataset or using another dataset like *Prima* or *DocBank* that includes those).
  * *Integration:* Using a PubLayNet model can be as easy as a few lines with LayoutParser in Python – for example: `lp.Detectron2LayoutModel(config_path='lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config', label_map={0:"Text",1:"Title",2:"List",3:"Table",4:"Figure"})` gives a ready-to-use predictor. This will output a list of layout elements with coordinates and labels, which you can then map to LaTeX structures. Alternatively, you can load weights in Detectron2 directly (weights are often shared via DropBox or HF Hub as in J.P. Leo’s repo). There are also other datasets for specific domains: *TableBank* for tables, *Newspaper Navigator* for newspaper layouts, *Prima* for printed layouts, etc., and LayoutParser offers models for those too. So if your documents differ (e.g., magazine-style layouts), a model fine-tuned on a closer domain could be swapped in. Overall, PubLayNet detectors are **easy to integrate and fast** (since they’re based on well-optimized CNN detectors), and they provide a strong baseline for layout analysis. They won’t capture textual semantics (they don’t know what the text says), but as a visual layout oracle they are very effective.
* **Other Noteworthy Models/Tools:**
  * **LayoutParser & DeepDetecton:** We mentioned LayoutParser’s pre-trained models for quick use. Another library,  *DeepDoctection* , also wraps layout detection and table recognition pipelines with pretrained models (using Detectron2 under the hood). These frameworks simplify using CV models for documents and often come with OCR integration as well.
  * **Table Structure Models:** If your conversion needs to not just detect tables but also convert their contents properly, you might consider models like Microsoft’s Table Transformer (which can recognize table row/column structure in addition to the table region). Table structure recognition can guide how to format LaTeX tabular environments (determining columns and multi-row cells). This goes beyond simple region detection but is useful if you have complex tables.
  * **Vision APIs:** Cloud services (Azure Form Recognizer, Google Document AI, Amazon Textract, etc.) use advanced layout models under the hood (often similar to the above). They can directly output JSON of layout elements (paragraphs, form fields, tables and their cells, etc.). These are proprietary but might be worth mentioning if integration effort needs to be minimal – however, since the focus here is on models and tools with pre-trained weights, the above open-source models are the primary candidates.

## Recommendations for a Layout “Oracle” in Word-to-LaTeX Conversion

To best leverage these models in a Word-to-LaTeX conversion pipeline, consider the following approach:

* **Use a Vision Model for Core Layout Zoning:** A high-accuracy detector like  **DiT (Document Image Transformer) with a PubLayNet-finetuned head** , or a  **Mask R-CNN PubLayNet model** , should form the backbone of your layout oracle. These will give you the bounding boxes of all significant regions on each page image along with their type (text, table, figure, etc.). For example, using the provided DiT PubLayNet model, you can identify that page 5 has two text columns, a figure at the top, and a table at the bottom with >95% accuracy[huggingface.co](https://huggingface.co/blog/document-ai#:~:text=Models%20that%20are%20currently%20state,a%20document%20is%20shown%20here). This visual analysis is more robust than relying solely on DOCX’s internal structure, especially for complex layouts (text boxes, multi-column formatting, etc. that might not translate directly in the DOCX XML).
* **Align Detected Regions with DOCX Content:** Since you have the original DOCX, you should use it to extract actual text, images, and figures (this avoids any OCR errors). The layout model’s output will guide how to group and order this content. For each detected region:
  * If the region is labeled  *Figure* : find the corresponding image in the DOCX (e.g., by spatial matching or by knowing the DOCX drawing object positions if available). Then, find any nearby text that looks like a caption (DOCX often has captions styled or placed right below images). A simple heuristic: if the layout model found a text region immediately below the figure and that text is small or italic or starts with "Figure", that’s your caption. You can then wrap the image in a LaTeX `figure` environment and include the caption text in `\caption{...}`. The model doesn’t explicitly label captions vs body text, but by proximity and perhaps with a LayoutLM-based classifier for “caption vs paragraph” on the text, you can accurately assign it. This ensures figures aren’t inlined as plain images but properly floated with their captions.
  * If the region is  *Table* : similarly, identify the corresponding table in DOCX. If the DOCX actually has that as a table object, you can directly convert it to a LaTeX `tabular` (preserving rows and columns). If the DOCX didn’t have an actual table (e.g., someone formatted it with tabs/spaces), the layout detector might still flag it as a table region visually. In that case, you could run a secondary algorithm or model to analyze lines and text alignment within that region to reconstruct rows and columns. Tools like Table Transformer or heuristic cell segmentation could help, or as a simpler approach, treat it as a preformatted block in LaTeX. Nevertheless, knowing it *is* a table is crucial so you wrap it in a `table` float and not as a normal paragraph. Include any detected title or footnotes of the table as `\caption` or table notes.
  * If the region is  *Text (paragraph)* : this will form the main body text. You should ensure the reading order follows the spatial order. For multi-column pages, the detector will output separate text regions for each column – you then know to merge the text from the left column first, then the right column (for correct reading order). In LaTeX, if the entire document is multi-column, you might use a two-column document class. If only certain pages are multi-column, you could use the `multicols` environment or manually set a two-column layout for that part. The key is the layout model informs you that “these text blocks are side by side horizontally”, which is a signal to preserve that formatting. Also, if some text regions are classified as *Title* or  *List* , you can use that: e.g., a region labeled "Title" could become a centered bold title in LaTeX or a section heading. A region labeled "List" might indicate you should convert a series of paragraphs into an `itemize` or `enumerate` environment. (PubLayNet’s "List" class is for things that look like lists with bullets or numbers.)
  * If the region is  *Header/Footer* : If your documents have running headers or footers, a generic model might not explicitly label them. You may need a custom step or model for this – for example, noticing repetitive text at the top/bottom of each page. You could train a small classifier on text segments (with features like position on page, font size from DOCX) to mark headers/footers. Once identified, you can either ignore them in the main body conversion (since LaTeX usually handles page headers/footers via `\maketitle` or fancyhdr package) or map them to appropriate LaTeX commands. For instance, if a header has the document title or chapter name, you might use `\markboth` to set running headers. Since the question focuses on CV models, just note that header/footer detection might be handled by a combination of layout position heuristics and possibly treating it as a separate “class” in a fine-tuned model if needed.
* **Leverage Multimodal Models for Edge Cases:** In most cases, the vision model plus DOCX content will suffice. However, if you have cases like *textboxes or sidebars* that are not clearly distinguished by the basic classes, a model like LayoutLMv3 or DocFormer could be fine-tuned to distinguish those based on textual and layout features. For example, a sidebar might have a different background color or border (a pure vision cue) or a certain style of text. A multimodal model can learn such patterns if you provide labeled examples (e.g., label some textboxes in your dataset). This would complement the generic detector. But if that’s not feasible, often simple rules can detect textboxes: DOCX parsing can tell if text is in a floating text box frame. You can then use the layout model’s coordinates to see if that frame corresponds to a distinct region. In LaTeX, a textbox could be rendered with a `minipage` or a floating environment with a framed box. The decision partly depends on how faithful the LaTeX output needs to be to the Word layout.
* **Pretrained Model Resources:** To implement the above, you can obtain several pre-trained models and code:
  * For a quick start, use **LayoutParser** with the PubLayNet Mask R-CNN model – it’s a few lines of code to get region predictions from an image. This could be integrated into your conversion script to get a list of regions per page.
  * If you need higher accuracy and are comfortable with a slightly more complex setup, use the  **DiT-based detector** . Microsoft’s UNILM GitHub provides code and a PubLayNet fine-tuned weight (CascadeRCNN + DiT-base) which you can clone and run. Hugging Face hosts a space with this model; while the space might require some fixes, the approach is documented, and you can replicate it locally. The improvement (mAP ~95% vs ~89% for the CNN model) might be worth it if your documents are particularly complex.
  * Keep **LayoutLMv3** in mind for any classification-refinement. There is a LayoutLMv3 model fine-tuned on PubLayNet as well (one report shows **95.1** mAP, similar to DiT’s performance). While LayoutLMv3 requires OCR text, it could potentially classify each detected region or even do detection itself. In practice, using it for detection isn’t as straightforward as using Detectron2, so it’s often used for tasks like segment classification. For instance, after initial detection, you could feed the text content of each region into a LayoutLM to classify it into more granular categories (maybe distinguishing a main body paragraph from a pull-quote textbox, if your layout model didn’t). This two-stage approach (detect via vision model, then classify via multimodal model) can combine the strengths of both.
  * All these models have **pretrained weights available** either on the Hugging Face Hub or via project repositories. We’ve cited links to many of them (e.g., Naver’s Donut, Microsoft’s LayoutLM and DiT, LayoutParser’s model zoo). This means you don’t need to train from scratch – you can fine-tune or directly apply these models to your documents. Ensure you check licenses (most research models are under permissive licenses for use).
* **Guiding LaTeX Generation:** Finally, integrate the oracle’s output into the LaTeX conversion logic. Essentially, the pipeline might look like:
  1. Export each page of the DOCX to an image (or PDF).
  2. Run the layout analysis model on the page image to get regions and categories.
  3. Map each region to content in the DOCX (using coordinates or index in reading order). You now have, for example, Region 1: Text (paragraph) containing “Introduction ...”, Region 2: Table containing a 3x4 array of text, Region 3: Figure containing Image X and caption “Figure 1: ...”.
  4. For each region, output corresponding LaTeX:
     * If Text: just write the paragraph text (or if it’s a list, you might need to detect list markers and use `\begin{itemize}`).
     * If Title/Heading: convert to `\section{...}` or appropriate level based on DOCX style.
     * If Figure: output a figure float with the image file and caption.
     * If Table: output a table float. You might need to convert the table’s internal structure: a simple approach is to use an HTML or Markdown table converter then adjust to LaTeX, or use a library if the DOCX table can be read. Since layout model only gives the boundary, rely on DOCX’s table data when possible.
     * If Header/Footer: possibly ignore or use fancyhdr to set them.
     * If Textbox/Sidebar: maybe treat as a float environment or a minipage with a border. LaTeX doesn’t have a direct analog of Word textboxes, but one can simulate with `\begin{figure}[!htb]` (for a floating sidebar) or packages like `tcolorbox`. The decision can be rule-based once you know a region is a textbox.
  5. Use the spatial order from the layout model to order these outputs. Usually, sorting regions top-to-bottom, left-to-right works (which the model can facilitate by giving coordinates). This prevents, say, a footer from appearing in the middle of the text or columns intermixing.

In conclusion, the most useful models for a layout oracle are those that **accurately detect region boundaries and categories** on document images. A PubLayNet-trained detector (such as one based on Mask R-CNN or the newer DiT transformer) is highly recommended as the first step, since it directly answers “where are the paragraphs, tables, figures on this page?”[huggingface.co](https://huggingface.co/blog/document-ai#:~:text=Models%20that%20are%20currently%20state,a%20document%20is%20shown%20here). For refining understanding (like identifying a caption vs a body paragraph), **LayoutLMv3** or similar multimodal models can be applied, leveraging textual cues. Tools like LayoutParser make integration easier, and pre-trained weights are available for all the mentioned models (LayoutLM v1-v3, Donut, DocFormer, PubLayNet detectors, DiT). By combining these, you gain a powerful layout-aware conversion pipeline: one that not only extracts content but also *knows* the role of each piece, enabling automatic decisions to produce idiomatic LaTeX (figures, tables, list environments, etc.). The end result is a LaTeX document that preserves the original Word document’s structure and formatting in a sound, automated way, achieved by state-of-the-art document AI models.

**Sources:**

1. Tamanna, *“Understanding LayoutLM,”* *Medium* (2025) – Overview of LayoutLM and its multimodal layout features.
2. Vijay Chaudhary, *“Document Intelligence: A Dive into LayoutLM…,”* *LinkedIn* (2025) – Describes how layout-aware Transformers (LayoutLM, DocFormer, etc.) recognize structures like tables, forms, and layout patterns.
3. Huang et al., *“LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking,”* arXiv (2022) – Proposed LayoutLMv3; unified Transformer for text+image, achieves SOTA on form understanding and layout analysis.
4. Rajiv Shah et al., *“Accelerating Document AI,”* Hugging Face Blog (Nov 2022) – Document AI use-cases and models. Notes that layout analysis is done via object detection; highlights LayoutLMv3 and DiT as state-of-the-art, with LayoutLMv3 reaching 0.951 mAP on PubLayNet[huggingface.co](https://huggingface.co/blog/document-ai#:~:text=Models%20that%20are%20currently%20state,a%20document%20is%20shown%20here).
5. Li et al., *“DiT: Self-supervised Pre-training for Document Image Transformer,”* arXiv (2022) – Introduces DiT model; demonstrates SOTA on RVL-CDIP classification, PubLayNet layout (94.9 mAP), and table detection.
6. PubLayNet paper – Xu et al. (2019), *“PubLayNet: largest dataset ever for document layout analysis.”* Describes the PubLayNet dataset (360K PDF pages, annotated as text, title, list, table, figure) and shows that deep detectors trained on it accurately recognize layouts[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/1908.07836#:~:text=In%20this%20work%2C%20we%20propose,dataset%20can%20be%20a%20more). The dataset and trained models improved transfer learning in document layout tasks.
7. LayoutParser Documentation – *Model Zoo* – Lists pre-trained layout detection models. Provides example usage of a PubLayNet Mask R-CNN (ResNet50/ResNeXt101) and reports their mAP performance[layout-parser.readthedocs.io](https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html#:~:text=lp%3A%2F%2FPubLayNet%2Fmask_rcnn_R_50_FPN_3x%2Fconfig%20PubLayNet%20mask_rcnn_X_101_32x8d_FPN_3x%20lp%3A%2F%2FPubLayNet%2Fmask_rcnn_X_101_32x8d_FPN_3x%2Fconfig%2088,csv%20NewspaperNavigator). Also shows the label taxonomy for PubLayNet (Text, Title, List, Table, Figure).
8. Naver Clova, *Donut: Document Understanding Transformer* – Model card and docs. Explains Donut’s OCR-free encoder-decoder approach with Swin and BART, for end-to-end document understanding. Available fine-tuned models (e.g., on DocVQA) illustrate its adaptability.
9. Appalaraju et al., *“DocFormer: End-to-End Transformer for Document Understanding,”* ICCV 2021 – Proposes DocFormer model. Achieves SOTA on multiple tasks (e.g., 96.17% on document classification, +0.58 F1 over LayoutLMv2 on forms) via multimodal encoder that fuses ResNet visuals with text tokens.
10. **(Additional)** Niels Rogge, *HF Spaces: Document Layout Analysis with DiT* – Demo implementation using DiT-base + Cascade Mask R-CNN on PubLayNet. Illustrates how to apply the model to an input page image (identifying text, title, list, table, figure regions).

![](https://www.google.com/s2/favicons?domain=https://layout-parser.readthedocs.io&sz=32)

![](https://www.google.com/s2/favicons?domain=https://ar5iv.labs.arxiv.org&sz=32)

![](https://www.google.com/s2/favicons?domain=https://huggingface.co&sz=32)

Sources
